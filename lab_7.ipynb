{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachin886x/deep-learning-lab/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec505ac-beb7-4ed7-9642-201e02548037",
      "metadata": {
        "id": "8ec505ac-beb7-4ed7-9642-201e02548037",
        "outputId": "d73a1d86-96aa-424e-fc03-66bf35cc0a7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Loading data...\n",
            "Train: 8000 | Val: 1000 | Test: 1000\n",
            "Src vocab: 2306 | Tgt vocab: 2927\n",
            "Parameters: 6,045,551\n",
            "Epoch 01 | Train Loss: 4.7471 | Val Loss: 3.8603 | Val PPL: 47.48\n",
            "Epoch 02 | Train Loss: 3.7744 | Val Loss: 3.4181 | Val PPL: 30.51\n",
            "Epoch 03 | Train Loss: 3.3625 | Val Loss: 3.1682 | Val PPL: 23.77\n",
            "Epoch 04 | Train Loss: 3.0519 | Val Loss: 2.9708 | Val PPL: 19.51\n",
            "Epoch 05 | Train Loss: 2.8038 | Val Loss: 2.8496 | Val PPL: 17.28\n",
            "Epoch 06 | Train Loss: 2.5867 | Val Loss: 2.7268 | Val PPL: 15.28\n",
            "Epoch 07 | Train Loss: 2.3851 | Val Loss: 2.6348 | Val PPL: 13.94\n",
            "Epoch 08 | Train Loss: 2.2042 | Val Loss: 2.5797 | Val PPL: 13.19\n",
            "Epoch 09 | Train Loss: 2.0347 | Val Loss: 2.4851 | Val PPL: 12.00\n",
            "Epoch 10 | Train Loss: 1.8767 | Val Loss: 2.4253 | Val PPL: 11.31\n",
            "Epoch 11 | Train Loss: 1.7289 | Val Loss: 2.4030 | Val PPL: 11.06\n",
            "Epoch 12 | Train Loss: 1.5894 | Val Loss: 2.3660 | Val PPL: 10.65\n",
            "Epoch 13 | Train Loss: 1.4561 | Val Loss: 2.3615 | Val PPL: 10.61\n",
            "Epoch 14 | Train Loss: 1.3415 | Val Loss: 2.3502 | Val PPL: 10.49\n",
            "Epoch 15 | Train Loss: 1.2207 | Val Loss: 2.3360 | Val PPL: 10.34\n",
            "Epoch 16 | Train Loss: 1.1133 | Val Loss: 2.3249 | Val PPL: 10.23\n",
            "Epoch 17 | Train Loss: 1.0145 | Val Loss: 2.3453 | Val PPL: 10.44\n",
            "Epoch 18 | Train Loss: 0.9184 | Val Loss: 2.3573 | Val PPL: 10.56\n",
            "Epoch 19 | Train Loss: 0.8376 | Val Loss: 2.3752 | Val PPL: 10.75\n",
            "Epoch 20 | Train Loss: 0.7019 | Val Loss: 2.3854 | Val PPL: 10.86\n",
            "\n",
            "Training time: 18.9 min\n",
            "Test Loss: 2.2158 | Test PPL: 9.17\n",
            "\n",
            "Calculating BLEU score (test set sample)...\n",
            "BLEU Score: 11.31\n",
            "\n",
            "--- Sample Translations ---\n",
            "  EN: Hello.\n",
            "  ES: <unk> .\n",
            "\n",
            "  EN: How are you?\n",
            "  ES: ¿ cómo estás ?\n",
            "\n",
            "  EN: I am fine.\n",
            "  ES: estoy <unk> .\n",
            "\n",
            "  EN: Good morning.\n",
            "  ES: buenos días .\n",
            "\n",
            "  EN: Thank you very much.\n",
            "  ES: gracias por ti .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Experiment 7: Sequence-to-Sequence Learning with Transformers\n",
        "English-to-Spanish Neural Machine Translation\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 0. Config\n",
        "# ─────────────────────────────────────────────\n",
        "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_PATH   = \"spa.txt\"          # tab-separated English\\tSpanish file\n",
        "MAX_SAMPLES = 10_000\n",
        "MAX_LEN     = 50\n",
        "MIN_FREQ    = 2\n",
        "BATCH_SIZE  = 64\n",
        "D_MODEL     = 256\n",
        "N_HEADS     = 8\n",
        "N_LAYERS    = 3\n",
        "D_FF        = 512\n",
        "DROPOUT     = 0.1\n",
        "EPOCHS      = 20\n",
        "LR          = 3e-4\n",
        "CLIP        = 1.0\n",
        "\n",
        "PAD, SOS, EOS, UNK = \"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 1. Data\n",
        "# ─────────────────────────────────────────────\n",
        "def load_pairs(path, max_samples=MAX_SAMPLES):\n",
        "    pairs = []\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "            if len(parts) >= 2:\n",
        "                pairs.append((parts[0].lower(), parts[1].lower()))\n",
        "    random.shuffle(pairs)\n",
        "    return pairs[:max_samples]\n",
        "\n",
        "def tokenize(sentence):\n",
        "    import re\n",
        "    return re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokens, min_freq=MIN_FREQ):\n",
        "        counter = Counter(tokens)\n",
        "        self.itos = [PAD, SOS, EOS, UNK] + [t for t, c in counter.items() if c >= min_freq]\n",
        "        self.stoi = {t: i for i, t in enumerate(self.itos)}\n",
        "    def __len__(self): return len(self.itos)\n",
        "    def encode(self, tokens):\n",
        "        return [self.stoi.get(t, self.stoi[UNK]) for t in tokens]\n",
        "\n",
        "def build_vocabs(pairs):\n",
        "    src_tokens = [t for en, _ in pairs for t in tokenize(en)]\n",
        "    tgt_tokens = [t for _, es in pairs for t in tokenize(es)]\n",
        "    return Vocab(src_tokens), Vocab(tgt_tokens)\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_vocab, tgt_vocab, max_len=MAX_LEN):\n",
        "        self.data = []\n",
        "        pad_idx = src_vocab.stoi[PAD]\n",
        "        for en, es in pairs:\n",
        "            src = src_vocab.encode(tokenize(en))[:max_len]\n",
        "            tgt = tgt_vocab.encode(tokenize(es))[:max_len]\n",
        "            self.data.append((src, tgt))\n",
        "\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, i): return self.data[i]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    def pad_seqs(seqs):\n",
        "        max_l = max(len(s) for s in seqs)\n",
        "        return torch.tensor([s + [0]*(max_l - len(s)) for s in seqs], dtype=torch.long)\n",
        "    src = pad_seqs(src_batch)\n",
        "    # tgt_in: <sos> + tokens, tgt_out: tokens + <eos>\n",
        "    tgt_in  = pad_seqs([[1] + list(t) for t in tgt_batch])\n",
        "    tgt_out = pad_seqs([list(t) + [2] for t in tgt_batch])\n",
        "    return src, tgt_in, tgt_out\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 2. Positional Encoding\n",
        "# ─────────────────────────────────────────────\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=DROPOUT, max_len=512):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 3. Multi-Head Attention\n",
        "# ─────────────────────────────────────────────\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=DROPOUT):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.h = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        B, L, _ = x.shape\n",
        "        return x.view(B, L, self.h, self.d_k).transpose(1, 2)  # (B, h, L, d_k)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        Q = self.split_heads(self.W_q(q))\n",
        "        K = self.split_heads(self.W_k(k))\n",
        "        V = self.split_heads(self.W_v(v))\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = self.dropout(F.softmax(scores, dim=-1))\n",
        "        out = (attn @ V).transpose(1, 2).contiguous()\n",
        "        out = out.view(out.size(0), out.size(1), -1)\n",
        "        return self.W_o(out)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 4. Feed Forward\n",
        "# ─────────────────────────────────────────────\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=DROPOUT):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 5. Encoder Layer & Encoder\n",
        "# ─────────────────────────────────────────────\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff   = FeedForward(d_model, d_ff, dropout)\n",
        "        self.ln1  = nn.LayerNorm(d_model)\n",
        "        self.ln2  = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.ln1(x + self.drop(self.attn(x, x, x, mask)))\n",
        "        x = self.ln2(x + self.drop(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.pe    = PositionalEncoding(d_model, dropout)\n",
        "        self.scale = math.sqrt(d_model)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        x = self.pe(self.embed(src) * self.scale)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 6. Decoder Layer & Decoder\n",
        "# ─────────────────────────────────────────────\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn  = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff  = FeedForward(d_model, d_ff, dropout)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ln3 = nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
        "        x = self.ln1(x + self.drop(self.self_attn(x, x, x, tgt_mask)))\n",
        "        x = self.ln2(x + self.drop(self.cross_attn(x, enc_out, enc_out, src_mask)))\n",
        "        x = self.ln3(x + self.drop(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.pe    = PositionalEncoding(d_model, dropout)\n",
        "        self.scale = math.sqrt(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, enc_out, src_mask, tgt_mask):\n",
        "        x = self.pe(self.embed(tgt) * self.scale)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, src_mask, tgt_mask)\n",
        "        return self.fc_out(x)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 7. Full Transformer\n",
        "# ─────────────────────────────────────────────\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size,\n",
        "                 d_model=D_MODEL, n_heads=N_HEADS, d_ff=D_FF,\n",
        "                 n_layers=N_LAYERS, dropout=DROPOUT):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, n_heads, d_ff, n_layers, dropout)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, n_heads, d_ff, n_layers, dropout)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        # (B, 1, 1, L)\n",
        "        return (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        B, L = tgt.shape\n",
        "        pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)          # (B,1,1,L)\n",
        "        causal   = torch.tril(torch.ones(L, L, device=tgt.device)).bool()  # (L,L)\n",
        "        return pad_mask & causal\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "        enc_out  = self.encoder(src, src_mask)\n",
        "        return self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 8. Training & Evaluation Utilities\n",
        "# ─────────────────────────────────────────────\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt_in, tgt_out in loader:\n",
        "        src, tgt_in, tgt_out = src.to(DEVICE), tgt_in.to(DEVICE), tgt_out.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src, tgt_in)           # (B, L, vocab)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt_in, tgt_out in loader:\n",
        "            src, tgt_in, tgt_out = src.to(DEVICE), tgt_in.to(DEVICE), tgt_out.to(DEVICE)\n",
        "            logits = model(src, tgt_in)\n",
        "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 9. Greedy Decoding\n",
        "# ─────────────────────────────────────────────\n",
        "def translate(model, sentence, src_vocab, tgt_vocab, max_len=MAX_LEN):\n",
        "    model.eval()\n",
        "    tokens = tokenize(sentence.lower())\n",
        "    src = torch.tensor([src_vocab.encode(tokens)], dtype=torch.long).to(DEVICE)\n",
        "    src_mask = model.make_src_mask(src)\n",
        "    enc_out  = model.encoder(src, src_mask)\n",
        "    tgt_ids  = [tgt_vocab.stoi[SOS]]\n",
        "    for _ in range(max_len):\n",
        "        tgt = torch.tensor([tgt_ids], dtype=torch.long).to(DEVICE)\n",
        "        tgt_mask = model.make_tgt_mask(tgt)\n",
        "        out = model.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
        "        next_id = out[0, -1].argmax().item()\n",
        "        tgt_ids.append(next_id)\n",
        "        if next_id == tgt_vocab.stoi[EOS]:\n",
        "            break\n",
        "    return \" \".join(tgt_vocab.itos[i] for i in tgt_ids[1:-1])\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 10. BLEU Score\n",
        "# ─────────────────────────────────────────────\n",
        "def bleu_score(model, pairs, src_vocab, tgt_vocab, n=200):\n",
        "    from collections import Counter\n",
        "    import math\n",
        "    refs, hyps = [], []\n",
        "    for en, es in random.sample(pairs, min(n, len(pairs))):\n",
        "        ref  = tokenize(es)\n",
        "        hyp  = tokenize(translate(model, en, src_vocab, tgt_vocab))\n",
        "        refs.append(ref)\n",
        "        hyps.append(hyp)\n",
        "\n",
        "    scores = []\n",
        "    for ngram in range(1, 5):\n",
        "        match, total = 0, 0\n",
        "        for ref, hyp in zip(refs, hyps):\n",
        "            ref_counts = Counter(tuple(ref[i:i+ngram]) for i in range(len(ref)-ngram+1))\n",
        "            hyp_grams  = [tuple(hyp[i:i+ngram]) for i in range(len(hyp)-ngram+1)]\n",
        "            for g in hyp_grams:\n",
        "                if ref_counts.get(g, 0) > 0:\n",
        "                    match += 1\n",
        "                    ref_counts[g] -= 1\n",
        "            total += len(hyp_grams)\n",
        "        scores.append(match / total if total > 0 else 0)\n",
        "\n",
        "    bp = min(1.0, sum(len(h) for h in hyps) / sum(len(r) for r in refs))\n",
        "    bleu = bp * math.exp(sum(math.log(s+1e-10) for s in scores) / 4)\n",
        "    return bleu * 100\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 11. Main\n",
        "# ─────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "\n",
        "    # Load & split data\n",
        "    print(\"Loading data...\")\n",
        "    pairs = load_pairs(DATA_PATH)\n",
        "    random.shuffle(pairs)\n",
        "    n = len(pairs)\n",
        "    train_pairs = pairs[:int(0.8*n)]\n",
        "    val_pairs   = pairs[int(0.8*n):int(0.9*n)]\n",
        "    test_pairs  = pairs[int(0.9*n):]\n",
        "    print(f\"Train: {len(train_pairs)} | Val: {len(val_pairs)} | Test: {len(test_pairs)}\")\n",
        "\n",
        "    # Build vocabs\n",
        "    src_vocab, tgt_vocab = build_vocabs(train_pairs)\n",
        "    print(f\"Src vocab: {len(src_vocab)} | Tgt vocab: {len(tgt_vocab)}\")\n",
        "\n",
        "    # Datasets & loaders\n",
        "    train_ds = TranslationDataset(train_pairs, src_vocab, tgt_vocab)\n",
        "    val_ds   = TranslationDataset(val_pairs,   src_vocab, tgt_vocab)\n",
        "    test_ds  = TranslationDataset(test_pairs,  src_vocab, tgt_vocab)\n",
        "    train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
        "    val_loader   = DataLoader(val_ds,   BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader  = DataLoader(test_ds,  BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Model\n",
        "    model = Transformer(len(src_vocab), len(tgt_vocab)).to(DEVICE)\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Parameters: {total_params:,}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float(\"inf\")\n",
        "    t0 = time.time()\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "        val_loss   = evaluate(model, val_loader, criterion)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_transformer.pt\")\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} \"\n",
        "              f\"| Val PPL: {math.exp(val_loss):.2f}\")\n",
        "\n",
        "    train_time = time.time() - t0\n",
        "    print(f\"\\nTraining time: {train_time/60:.1f} min\")\n",
        "\n",
        "    # Load best & evaluate on test\n",
        "    model.load_state_dict(torch.load(\"best_transformer.pt\", map_location=DEVICE))\n",
        "    test_loss = evaluate(model, test_loader, criterion)\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test PPL: {math.exp(test_loss):.2f}\")\n",
        "\n",
        "    print(\"\\nCalculating BLEU score (test set sample)...\")\n",
        "    bleu = bleu_score(model, test_pairs, src_vocab, tgt_vocab)\n",
        "    print(f\"BLEU Score: {bleu:.2f}\")\n",
        "\n",
        "    # Sample translations\n",
        "    print(\"\\n--- Sample Translations ---\")\n",
        "    examples = [\n",
        "        \"Hello.\",\n",
        "        \"How are you?\",\n",
        "        \"I am fine.\",\n",
        "        \"Good morning.\",\n",
        "        \"Thank you very much.\",\n",
        "    ]\n",
        "    for sent in examples:\n",
        "        print(f\"  EN: {sent}\")\n",
        "        print(f\"  ES: {translate(model, sent, src_vocab, tgt_vocab)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c854c092-c312-478c-9a9d-69557cfb59cb",
      "metadata": {
        "id": "c854c092-c312-478c-9a9d-69557cfb59cb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
