{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sachin886x/deep-learning-lab/blob/main/lab_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa64f7ae-1cae-4b67-baec-5963ff9dafb4",
      "metadata": {
        "id": "fa64f7ae-1cae-4b67-baec-5963ff9dafb4",
        "outputId": "f17b7e59-c20f-4cab-b7ab-96a3ca7d4ea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d94ae7a-512b-449b-8791-a964554e3afa",
      "metadata": {
        "id": "8d94ae7a-512b-449b-8791-a964554e3afa"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv(\"poems-100 - poems-100.csv\")\n",
        "\n",
        "text = \" \".join(data[\"text\"].astype(str)).lower()\n",
        "words = text.split()\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = sorted(set(words))\n",
        "word_to_idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx_to_word = {i:w for w,i in word_to_idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 5\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(len(words) - seq_length):\n",
        "    X.append([word_to_idx[w] for w in words[i:i+seq_length]])\n",
        "    y.append(word_to_idx[words[i+seq_length]])\n",
        "\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)   # small batch size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99b61287-cab7-47e7-9e1e-e1ad528cc033",
      "metadata": {
        "id": "99b61287-cab7-47e7-9e1e-e1ad528cc033"
      },
      "outputs": [],
      "source": [
        "class OneHotRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.one_hot(x, num_classes=vocab_size).float()\n",
        "        x = x.to(device)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99962350-22d2-453b-b901-006bb974e393",
      "metadata": {
        "id": "99962350-22d2-453b-b901-006bb974e393"
      },
      "outputs": [],
      "source": [
        "class EmbeddingRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4390144e-9638-41d5-91bc-e55b420d1c2e",
      "metadata": {
        "id": "4390144e-9638-41d5-91bc-e55b420d1c2e"
      },
      "outputs": [],
      "source": [
        "def train_model(model, epochs=10):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for inputs, targets in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e54ab0-f00c-44e0-bdee-234be0c215c8",
      "metadata": {
        "id": "22e54ab0-f00c-44e0-bdee-234be0c215c8",
        "outputId": "2ff28e9b-9e72-4ff0-fcb6-9a065805aaff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training One-Hot RNN\n",
            "Epoch 1/20, Loss: 7.3660\n",
            "Epoch 2/20, Loss: 6.1605\n",
            "Epoch 3/20, Loss: 4.8593\n",
            "Epoch 4/20, Loss: 3.2055\n",
            "Epoch 5/20, Loss: 1.6560\n",
            "Epoch 6/20, Loss: 0.7567\n",
            "Epoch 7/20, Loss: 0.4564\n",
            "Epoch 8/20, Loss: 0.3461\n",
            "Epoch 9/20, Loss: 0.2810\n",
            "Epoch 10/20, Loss: 0.2434\n",
            "Epoch 11/20, Loss: 0.2291\n",
            "Epoch 12/20, Loss: 0.2112\n",
            "Epoch 13/20, Loss: 0.2114\n",
            "Epoch 14/20, Loss: 0.1841\n",
            "Epoch 15/20, Loss: 0.1874\n",
            "Epoch 16/20, Loss: 0.1836\n",
            "Epoch 17/20, Loss: 0.1662\n",
            "Epoch 18/20, Loss: 0.1841\n",
            "Epoch 19/20, Loss: 0.1565\n",
            "Epoch 20/20, Loss: 0.1688\n",
            "\n",
            "Training Embedding RNN\n",
            "Epoch 1/20, Loss: 7.5379\n",
            "Epoch 2/20, Loss: 6.2877\n",
            "Epoch 3/20, Loss: 4.9458\n",
            "Epoch 4/20, Loss: 3.8443\n",
            "Epoch 5/20, Loss: 3.0660\n",
            "Epoch 6/20, Loss: 2.5496\n",
            "Epoch 7/20, Loss: 2.1802\n",
            "Epoch 8/20, Loss: 1.9390\n",
            "Epoch 9/20, Loss: 1.7676\n",
            "Epoch 10/20, Loss: 1.6537\n",
            "Epoch 11/20, Loss: 1.5538\n",
            "Epoch 12/20, Loss: 1.4731\n",
            "Epoch 13/20, Loss: 1.4323\n",
            "Epoch 14/20, Loss: 1.4110\n",
            "Epoch 15/20, Loss: 1.3584\n",
            "Epoch 16/20, Loss: 1.3473\n",
            "Epoch 17/20, Loss: 1.3483\n",
            "Epoch 18/20, Loss: 1.3526\n",
            "Epoch 19/20, Loss: 1.3197\n",
            "Epoch 20/20, Loss: 1.3174\n"
          ]
        }
      ],
      "source": [
        "print(\"Training One-Hot RNN\")\n",
        "onehot_model = OneHotRNN(vocab_size, hidden_size=128)\n",
        "train_model(onehot_model, epochs=20)\n",
        "\n",
        "print(\"\\nTraining Embedding RNN\")\n",
        "embedding_model = EmbeddingRNN(vocab_size, embed_dim=128, hidden_size=128)\n",
        "train_model(embedding_model, epochs=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630eb39f-8b14-4180-9e0d-cc52122a5630",
      "metadata": {
        "id": "630eb39f-8b14-4180-9e0d-cc52122a5630"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_word, length=20):\n",
        "    model.eval()\n",
        "    result = [start_word]\n",
        "    current = torch.tensor([[word_to_idx[start_word]]]).to(device)\n",
        "\n",
        "    for _ in range(length):\n",
        "        output = model(current)\n",
        "        prob = torch.softmax(output, dim=1)\n",
        "        next_idx = torch.multinomial(prob, 1).item()\n",
        "\n",
        "        result.append(idx_to_word[next_idx])\n",
        "        current = torch.tensor([[next_idx]]).to(device)\n",
        "\n",
        "    return \" \".join(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebe32ffa-9c6b-4420-ac21-5144114ce605",
      "metadata": {
        "id": "ebe32ffa-9c6b-4420-ac21-5144114ce605",
        "outputId": "147509f0-1090-489e-83dd-1e073afae494"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "love thee to be of the work and the work but i tell have learnt men and i know who would\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(embedding_model, \"love\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c88417-0398-41d6-afb3-42c3e30614a3",
      "metadata": {
        "id": "f8c88417-0398-41d6-afb3-42c3e30614a3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}